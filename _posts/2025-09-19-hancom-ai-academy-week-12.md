---
title: "[스나이퍼팩토리] 한컴AI 2기 - 교육 12주차 후기"
description: >-
   Pandas 의 핵심 기능(데이터 구조, I/O, 정제, GroupBy, 시계열) 학습 및 타이타닉 데이터셋 분석을 통한 데이터 분석 워크플로우 실습.
author: gisoun
date: 2025-09-19 10:18:00 +0900
categories: [Hancom AI Academy, Education]
tags: [hancom, hancom ai academy, education, ai, python, pandas, data analysis]
pin: false
media_subpath: '/assets/posts/20250919'
published: true
---

> 2025\. 9\. 15\. ~ 2025\. 9\. 19\.

---

12주차에는 Python 데이터 분석의 핵심 라이브러리인 **Pandas** 의 기능 전반을 학습하는 시간이었습니다. 데이터의 기본 구조부터 시작하여 파일 입출력, 데이터 정제, 그룹 연산, 시계열 분석, 그리고 타이타닉 데이터셋을 활용한 시각화 실습, 데이터 분석 워크플로우의 전 과정을 체계적으로 다루었습니다.

---

## Pandas

**Pandas** 를 활용한 데이터 분석은 **Series** 와 **DataFrame** 이라는 두 가지 핵심 데이터 구조에 대한 이해에서 출발합니다.

### Series

**Series** 는 인덱스(index)를 가진 **1차원 배열**과 유사한 객체입니다. 리스트, NumPy 배열, 딕셔너리 등 다양한 데이터 소스로부터 생성할 수 있습니다.

```py
import pandas as pd
import numpy as np

# 리스트로부터 Series 생성
s = pd.Series([4, 7, -5, 3])
print(s)

# 인덱스 지정하여 Series 생성
s2 = pd.Series([4, 7, -5, 3], index=['d', 'b', 'a', 'c'])
print(s2)
```

**Series** 는 인덱스 라벨을 사용한 데이터 선택, 필터링, 벡터화된 연산 등 NumPy 배열의 기능을 대부분 지원하면서도, 각 데이터 포인트에 명시적인 이름을 부여할 수 있다는 장점이 있습니다.

### DataFrame

**DataFrame** 은 행과 열에 각각 인덱스가 있는 2차원 테이블 형태의 자료구조입니다. 각 열은 서로 다른 데이터 타입을 가질 수 있으며, 동일한 길이를 가진 리스트나 NumPy 배열로 구성된 딕셔너리를 통해 생성하는 것이 가장 일반적입니다.

```py
# 딕셔너리로부터 DataFrame 생성
data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada', 'Nevada'],
        'year': [2000, 2001, 2002, 2001, 2002, 2003],
        'pop': [1.5, 1.7, 3.6, 2.4, 2.9, 3.2]}
df = pd.DataFrame(data)
```

### 데이터 선택 및 필터링

**Pandas** 는 데이터 선택을 위해 라벨 기반의 **`loc`** 와 정수 위치 기반의 **`iloc`** 라는 두 가지 핵심 인덱서를 제공합니다. 이들을 사용하면 특정 행과 열의 데이터를 정교하게 선택할 수 있습니다.

```py
# 'Ohio' 행의 'year', 'pop' 열 선택 (라벨 기반)
df.loc[df['state'] == 'Ohio', ['year', 'pop']]

# 첫 3개 행의 첫 2개 열 선택 (위치 기반)
df.iloc[:3, :2]
```

---

## 데이터 입출력(I/O)

데이터 분석은 다양한 소스로부터 데이터를 불러오는 것으로 시작됩니다. **Pandas** 는 여러 파일 포맷을 지원하는 강력한 I/O 함수들을 제공합니다.

### CSV 및 텍스트 파일 읽기

**`pd.read_csv()`** 는 CSV 파일을 **DataFrame** 으로 불러오는 가장 기본적인 함수입니다. 다양한 옵션을 통해 복잡한 형식의 파일도 처리할 수 있습니다.

- **`filepath_or_buffer`**: 파일 경로 또는 URL.
- **`sep`**: 필드를 구분하는 구분자 (기본값: **`,`**).
- **`header`**: 열 이름으로 사용할 행 번호 (기본값: 0). 헤더가 없으면 **`None`** 으로 지정.
- **`names`**: 헤더가 없을 경우 사용할 열 이름 리스트.
- **`index_col`**: 인덱스로 사용할 열의 이름 또는 번호.

```py
# 헤더가 없는 CSV 파일 불러오기
df_no_header = pd.read_csv('ex2.csv', header=None)

# 열 이름을 직접 지정하고 message 열을 인덱스로 사용
df_with_index = pd.read_csv('ex2.csv', names=['a', 'b', 'c', 'd', 'message'], index_col='message')
```

### Excel, JSON 등 다양한 파일 형식 다루기

**Pandas** 는 **`pd.read_excel()`**, **`pd.read_json()`**, **`pd.read_sql()`** 등 특정 포맷에 최적화된 함수를 제공하여 분석가가 다양한 데이터 소스를 쉽게 통합할 수 있도록 지원합니다. 대용량 데이터 처리를 위해 **Parquet**, **HDF5**와 같은 효율적인 바이너리 포맷도 지원합니다.

### 데이터 저장하기

분석이 완료된 **DataFrame** 은 **`to_csv()`**, **`to_excel()`** 등의 메서드를 사용하여 파일로 저장할 수 있습니다. **`index=False`** 옵션으로 인덱스를 제외하거나, **`columns`** 옵션으로 특정 열만 선택하여 저장하는 등 세부적인 제어가 가능합니다.

```py
# 인덱스를 제외하고 '|'를 구분자로 사용하여 파일로 저장
df.to_csv('output.csv', index=False, sep='|')
```

---

## 데이터 정제 및 전처리

실제 데이터는 결측치, 중복, 비일관적인 형식 등 여러 문제를 포함하고 있습니다. 정확한 분석을 위해서는 데이터를 정제하고 전처리하는 과정이 필수적입니다.

### 결측치(Missing Data) 처리

결측치(NaN, Not a Number)는 데이터 분석의 가장 큰 장애물 중 하나입니다.

- **`isnull().sum()`**: 각 열의 결측치 개수를 확인합니다.
- **`dropna()`**: 결측치가 포함된 행이나 열을 제거합니다. **`how='all'`** 옵션은 모든 값이 결측치인 경우에만 제거합니다.
- **`fillna()`**: 결측치를 특정 값으로 대체합니다. 상수, 딕셔너리(열마다 다른 값 지정), 또는 **`method='ffill'`**(앞의 값으로 채우기) 등의 옵션을 사용할 수 있습니다.

```py
# 'Age' 열의 결측치를 평균 나이로 채우기 (타이타닉 예제)
mean_age = titanic_df['Age'].mean()
titanic_df['Age'].fillna(mean_age, inplace=True)
```

### 중복 데이터 제거

**`duplicated()`** 는 중복된 행을 **`boolean Series`** 로 반환하며, **`drop_duplicates()`**는 중복된 행을 제거한 새로운 **DataFrame** 을 반환합니다. **`subset`** 인자를 사용하여 특정 열을 기준으로 중복을 검사할 수 있습니다.

### 데이터 변환 기법

- **`값 매핑 및 대체`**: **`map()`** 은 **Series** 의 각 값에 함수를 적용하거나 딕셔너리를 통해 값을 매핑합니다. **`replace()`** 는 특정 값을 다른 값으로 대체하는 데 사용됩니다.
- **`구간화(Binning)`**: **`pd.cut()`** 은 연속형 데이터를 지정된 구간(bin)으로 나눕니다. **`pd.qcut()`** 은 데이터의 분위수를 기준으로 동일한 개수의 데이터가 들어가도록 구간을 나눕니다.
- **`문자열 조작`**: **`.str`** 접근자를 사용하면 **`contains()`**, **`split()`**, **`replace()`**, **`extract()(정규표현식)`** 등 다양한 문자열 메서드를 **Series** 의 모든 요소에 일괄적으로 적용할 수 있습니다.

---

## 데이터 집계와 그룹 연산

데이터를 특정 기준에 따라 그룹으로 묶고, 각 그룹에 함수를 적용하여 통계를 내는 것은 데이터 분석의 핵심적인 과정입니다.

### GroupBy

**Pandas** 의 **`groupby()`** 메서드는 **'분할-적용-결합(Split-Apply-Combine)' 패러다임**을 구현합니다.

- **`분할(Split)`**: **`df.groupby('key')`** 와 같이 특정 키를 기준으로 데이터를 그룹으로 나눕니다.
- **`적용(Apply)`**: 각 그룹에 집계 함수(**`sum`**, **`mean`**, **`count`** 등), 변환 함수(**`transform`**), 또는 사용자 정의 함수(**`apply`**)를 적용합니다.
- **`결합(Combine)`**: 각 그룹의 결과를 새로운 데이터 구조로 통합합니다.

```py
# 타이타닉 데이터에서 좌석 등급(Pclass)별 생존율 계산
pclass_survival = titanic_df.groupby('Pclass')['Survived'].mean()
```

### 데이터 집계(Aggregation)

**`agg()`** 메서드를 사용하면 여러 집계 함수를 동시에 적용하거나, 각 열에 서로 다른 함수를 적용하는 복잡한 집계가 가능합니다.

```py
# 타이타닉 데이터에서 그룹별로 여러 통계량 계산
grouped = titanic_df.groupby(['Sex', 'Pclass'])
grouped['Survived'].agg(['mean', 'count'])
```

### 데이터 변환(Transformation)

**`transform()`** 은 그룹별 계산 결과를 원본 데이터와 동일한 형태로 반환하여, 그룹 내 표준화 등의 작업에 유용합니다.

### 피벗 테이블과 교차표

- **`pivot_table()`**: 데이터를 재구성하여 다차원 요약 테이블을 생성합니다. **`index`**, **`columns`**, **`values`**, **`aggfunc`** 등의 인자를 사용합니다.
- **`crosstab()`**: 두 변수 간의 빈도를 계산하는 교차표를 생성합니다.

---

## 시계열 데이터 분석

**Pandas** 는 금융, 경제 데이터 분석에 필수적인 시계열 데이터 처리 기능을 강력하게 지원합니다.

### 시계열 데이터 구조

- **`Timestamp와 DatetimeIndex`**: **`pd.to_datetime()`** 으로 문자열을 날짜 타입으로 변환하거나, **`pd.date_range()`** 로 특정 기간의 **`DatetimeIndex`** 를 생성할 수 있습니다.
- **`시간 기반 인덱싱`**: **`ts['2023-01']`** 과 같이 부분적인 날짜 문자열로 데이터를 쉽게 슬라이싱할 수 있습니다.

### 리샘플링

**`resample()`** 메서드는 시계열 데이터의 빈도를 변경하는 기능입니다.

- **`다운샘플링`**: 일별 데이터를 월별 데이터로 집계하는 경우 (**`ts.resample('M').sum()`**).
- **`업샘플링`**: 월별 데이터를 일별 데이터로 변환하는 경우, **`ffill()`** 등으로 결측치를 보간해야 합니다.

### 이동창 함수

**`rolling()`** 메서드는 고정된 크기의 창을 데이터 위로 이동시키면서 통계를 계산합니다. 주식 데이터의 이동 평균(**`rolling(window=30).mean()`**) 등을 계산하여 추세를 분석하는 데 사용됩니다.

---

## 타이타닉 생존자 데이터 분석

이번 12주차 학습 내용을 종합하여 **타이타닉 데이터셋 분석** 실습을 진행했습니다. 이 과정은 데이터 분석의 전체 워크플로우를 실습하는 좋은 기회였습니다.

### 데이터 로딩 및 탐색

**`pd.read_csv`** 로 데이터를 로드하고, **`info()`**, **`describe()`**, **`isnull().sum()`** 을 통해 데이터의 구조, 통계치, 결측치를 파악했습니다.

### 데이터 정제 및 특성 공학

결측치가 많은 **`Age`**, **`Embarked`** 열을 **`fillna`** 를 이용해 각각 평균값과 최빈값으로 채웠습니다. 또한, **`SibSp`** 와 **`Parch`** 열을 조합하여 **`FamilySize`** 와 **`IsAlone`** 이라는 새로운 파생 변수를 생성했습니다.

### 탐색적 데이터 분석(EDA) 및 시각화

**`groupby`** 를 이용해 성별, 좌석 등급, 연령대 등 다양한 변수와 생존율 간의 관계를 분석했습니다. **`matplotlib`** 과 **`seaborn`** 라이브러리를 활용하여 분석 결과를 바 차트, 히트맵 등으로 시각화하여 인사이트를 명확하게 전달했습니다.

```py
# 성별과 좌석 등급에 따른 생존율 교차 분석
survival_by_sex_class = titanic_df.groupby(['Sex', 'Pclass'])['Survived'].mean().unstack()

# 히트맵 시각화
import seaborn as sns
import matplotlib.pyplot as plt

sns.heatmap(survival_by_sex_class, annot=True, cmap='Blues', fmt='.3f')
plt.title('Survival Rate by Sex & Pclass')
plt.show()
```

---

## 마무리

12주차는 **Pandas** 의 방대한 기능을 깊이 있게 탐색하며 데이터 분석의 기초 체력을 다지는 중요한 시간이었습니다. 데이터의 구조를 이해하는 것부터 시작해, 현실의 지저분한 데이터를 정제하고, **`groupby`** 를 활용해 숨겨진 패턴을 찾아내며, 시계열 데이터의 특성을 다루는 법까지 학습했습니다.  

특히 타이타닉 데이터셋 실습을 통해 이론으로 배운 기능들을 실제 분석 과정에 어떻게 유기적으로 연결하고 활용하는지에 대한 실질적인 감각을 익힐 수 있었습니다.

---

본 후기는 [한글과컴퓨터x한국생산성본부x스나이퍼팩토리] 한컴 AI 아카데미 2기 (B-log) 리뷰로 작성 되었습니다.

#한컴AI아카데미2기 #AI개발자 #AI개발자교육 #한글과컴퓨터 #한국생산성본부 #스나이퍼팩토리 #부트캠프 #AI전문가양성 #개발자교육 #개발자취업
